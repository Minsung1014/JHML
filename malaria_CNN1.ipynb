{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlnjMEhSz8DusrgvU0jVzS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minsung1014/JHML/blob/main/malaria_CNN1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "819JqN8aiVf0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "# Set the `numpy` pseudo-random generator at a fixed value\n",
        "# This helps with repeatable results everytime you run the code.\n",
        "np.random.seed(1000)\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import keras\n",
        "\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'  # Added to set the backend as Tensorflow\n",
        "# We can also set it to Theano if we want.\n",
        "\n",
        "\n",
        "# Iterate through all images in Parasitized folder, resize to 64 x 64\n",
        "# Then save as numpy array with name 'dataset'\n",
        "# Set the label to this as 0\n",
        "#classifications = 2\n",
        "#epochs=5\n",
        "#image_directory = 'D:/NPU_2020/Course/Joint_Human_Machine_Learning/2023/JHML/deep learning/cell_images/'\n",
        "\n",
        "image_directory = 'D:/NPU_2020/Course/Joint_Human_Machine_Learning/2024/JHML/deep learning/cell_images/'\n",
        "\n",
        "SIZE = 64\n",
        "dataset = []  # Many ways to handle data, you can use pandas. Here, we are using a list format.\n",
        "label = []  # Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.\n",
        "\n",
        "parasitized_images = os.listdir(image_directory + 'Parasitized/')\n",
        "for i, image_name in enumerate(\n",
        "        parasitized_images):  # Remember enumerate method adds a counter and returns the enumerate object\n",
        "\n",
        "    if (image_name.split('.')[1] == 'png'):\n",
        "        image = cv2.imread(image_directory + 'Parasitized/' + image_name)\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        dataset.append(np.array(image))\n",
        "        label.append(0)\n",
        "\n",
        "# Iterate through all images in Uninfected folder, resize to 64 x 64\n",
        "# Then save into the same numpy array 'dataset' but with label 1\n",
        "\n",
        "uninfected_images = os.listdir(image_directory + 'Uninfected/')\n",
        "for i, image_name in enumerate(uninfected_images):\n",
        "    if (image_name.split('.')[1] == 'png'):\n",
        "        image = cv2.imread(image_directory + 'Uninfected/' + image_name)\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        dataset.append(np.array(image))\n",
        "        label.append(1)\n",
        "#ticklabels =['Parasitized','Uninfected']\n",
        "# Apply CNN\n",
        "# ### Build the model\n",
        "\n",
        "#############################################################\n",
        "###2 conv and pool layers. with some normalization and drops in between.\n",
        "\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = None\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(32, (3, 3), input_shape = (SIZE, SIZE, 3), activation = 'relu', data_format='channels_last'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2), data_format=\"channels_last\"))\n",
        "model.add(BatchNormalization(axis = -1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Convolution2D(32, (3, 3), activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2), data_format=\"channels_last\"))\n",
        "model.add(BatchNormalization(axis = -1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(activation = 'relu', units=512))\n",
        "model.add(BatchNormalization(axis = -1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(activation = 'relu', units=256))\n",
        "model.add(BatchNormalization(axis = -1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(activation = 'sigmoid', units=2))\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Save the model\n",
        "model.save('malaria_cnn.h5')\n",
        "###############################################################\n",
        "\n",
        "### Split the dataset\n",
        "#\n",
        "# I split the dataset into training and testing dataset.\n",
        "# 1. Training data: 80%\n",
        "# 2. Testing data: 20%\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, to_categorical(np.array(label)), test_size=0.20,\n",
        "                                                    random_state=0)\n",
        "\n",
        "# When training with Keras's Model.fit(), adding the tf.keras.callback.TensorBoard callback\n",
        "# ensures that logs are created and stored. Additionally, enable histogram computation\n",
        "# every epoch with histogram_freq=1 (this is off by default)\n",
        "# Place the logs in a timestamped subdirectory to allow easy selection of different training runs.\n",
        "\n",
        "# import datetime\n",
        "\n",
        "# log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"/\"\n",
        "# tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "# ### Training the model\n",
        "# As the training data is now ready, I will use it to train the model.\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(np.array(X_train),\n",
        "                    y_train,\n",
        "                    batch_size=64,\n",
        "                    verbose=1,\n",
        "                    epochs=30,  # Changed to 3 from 50 for testing purposes.\n",
        "                    validation_split=0.1,\n",
        "                    shuffle=False\n",
        "                    #   callbacks=callbacks\n",
        "                    )\n",
        "\n",
        "# ## Accuracy calculation\n",
        "#\n",
        "# I'll now calculate the accuracy on the test data.\n",
        "\n",
        "print(\"Test_Accuracy: {:.2f}%\".format(model.evaluate(np.array(X_test), np.array(y_test))[1] * 100))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}